{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to change the paths of your object detection file, it's too big to put on git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#840\n",
    "\n",
    "from imutils.video import WebcamVideoStream\n",
    "from imutils.video import FPS\n",
    "import os\n",
    "import imutils\n",
    "from IPython.display import display\n",
    "from cv2 import *\n",
    "from cv2 import imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15\n",
    "#!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                             2.0.0      \n",
      "tensorflow-estimator                   2.0.1      \n",
      "tensorflow-object-detection-api        0.1.1      \n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.00\n",
    "!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/guilhermeviveiros/Desktop/models/research/object_detection\"\n",
    "MODEL_NAME = 'hand_graph'\n",
    "PATH_TO_CKPT = path + '/' + MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join(path+'/training', 'labelmap.pbtxt')\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.compat.v1.get_default_graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v2.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "label_map_util.tf = tf.compat.v1\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_several_images(PATH_TO_TEST_IMAGES_DIR = \"/Users/guilhermeviveiros/Desktop/Part_01/469\"):\n",
    "    \n",
    "    TEST_IMAGE_PATHS = []\n",
    "\n",
    "    for i in range(1,33):\n",
    "        if(i < 10):\n",
    "            path = os.path.join(PATH_TO_TEST_IMAGES_DIR, '0000{}.jpg'.format(i))\n",
    "            TEST_IMAGE_PATHS.append(path)\n",
    "        else:\n",
    "            path = os.path.join(PATH_TO_TEST_IMAGES_DIR, '000{}.jpg'.format(i))\n",
    "            TEST_IMAGE_PATHS.append(path)      \n",
    "                            \n",
    "    frames = []\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            for image_path in TEST_IMAGE_PATHS:\n",
    "               \n",
    "                image = cv2.imread(image_path)\n",
    "            \n",
    "                image = imutils.resize(image, width=600)\n",
    "                #image_np = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                image = cv2.Canny(image,100,200) \n",
    "                \n",
    "               \n",
    "                image_np_expanded = np.expand_dims(image_np,axis=0)\n",
    "                image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "                boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "                scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "                classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "                num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "                (boxes, scores, classes, num_detections) = sess.run(\n",
    "                    [boxes, scores, classes, num_detections],\n",
    "                    feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "                # Visualization of the results of a detection.\n",
    "                vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                          image_np,\n",
    "                          np.squeeze(boxes),\n",
    "                          np.squeeze(classes).astype(np.int32),\n",
    "                          np.squeeze(scores),\n",
    "                          category_index,\n",
    "                          use_normalized_coordinates=True,\n",
    "                          max_boxes_to_draw = 1,\n",
    "                          min_score_thresh=0.7,\n",
    "                          line_thickness=8)\n",
    "            \n",
    "                \n",
    "\n",
    "                frames.append(image)\n",
    "        \n",
    "            save_frames(frames)\n",
    "\n",
    "##if you wanna test the model in several images run the function bellow but change the path            \n",
    "##test_several_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames):\n",
    "    \n",
    "    for (step,frame) in enumerate(frames):\n",
    "        \n",
    "        if step < 10:\n",
    "            imwrite(\"/Users/guilhermeviveiros/Desktop/LEI/Task1/Fotos/0\" + str(step) + \".jpg\", frame)\n",
    "        else:\n",
    "            imwrite(\"/Users/guilhermeviveiros/Desktop/LEI/Task1/Fotos/\" + str(step) + \".jpg\", frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENCV WITHOUT THREADING FOR CAPTURING \n",
    "## grab a pointer to the video stream and initialize the FPS counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# created a *threaded* video stream, allow the camera sensor to warmup,\n",
    "# and start the FPS counter\n",
    "\n",
    "\n",
    "print(\"[INFO] sampling THREADED frames from webcam...\")\n",
    "vs = cv2.VideoCapture(0)\n",
    "i = 0\n",
    "\n",
    "\n",
    "#with detection_graph.as_default():\n",
    "#        with tf.Session(graph=detection_graph) as sess:\n",
    "#g = tf.Graph()\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    \n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        while True:\n",
    "            \n",
    "            ret,frame = vs.read()\n",
    "            frame = cv2.flip(frame,1)\n",
    "            frame = imutils.resize(frame, width=600)\n",
    "            #frame = cv2.resize(frame, (400,400))\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "            image_np = frame\n",
    "            #edges = cv2.Canny(frame,100,200) \n",
    "            image_np_expanded = np.expand_dims(image_np,axis=0)\n",
    "    \n",
    "            \n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            \n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "            [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "                    \n",
    "                                    \n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw = 2,\n",
    "                min_score_thresh=0.9,\n",
    "                line_thickness=8)\n",
    "                    \n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "                    \n",
    "                \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        # do a bit of cleanup\n",
    "        #vs.stream.release()\n",
    "        vs.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cunny Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV program to perform Edge detection in real time \n",
    "# import libraries of python OpenCV  \n",
    "# where its functionality resides \n",
    "import cv2  \n",
    "  \n",
    "# np is an alias pointing to numpy library \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# capture frames from a camera \n",
    "cap = cv2.VideoCapture(0) \n",
    "  \n",
    "  \n",
    "# loop runs if capturing has been initialized \n",
    "while(1): \n",
    "  \n",
    "    # reads frames from a camera \n",
    "    ret, frame = cap.read() \n",
    "  \n",
    "    # converting BGR to HSV \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) \n",
    "      \n",
    "    # define range of red color in HSV \n",
    "    lower_red = np.array([30,150,50]) \n",
    "    upper_red = np.array([255,255,180]) \n",
    "      \n",
    "    # create a red HSV colour boundary and  \n",
    "    # threshold HSV image \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red) \n",
    "  \n",
    "    # Bitwise-AND mask and original image \n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask) \n",
    "  \n",
    "    # Display an original image \n",
    "    cv2.imshow('Original',frame) \n",
    "    frame = cv2.flip(frame,1)\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "        \n",
    "    # finds edges in the input image image and \n",
    "    # marks them in the output map edges \n",
    "    edges = cv2.Canny(frame,100,200) \n",
    "  \n",
    "    # Display edges in a frame \n",
    "    cv2.imshow('Edges',edges) \n",
    "  \n",
    "    # Wait for Esc key to stop \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "                    \n",
    "  \n",
    "  \n",
    "# Close the window \n",
    "cap.release() \n",
    "  \n",
    "# De-allocate any associated memory usage \n",
    "cv2.destroyAllWindows()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just an example of our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Ready to start\n"
     ]
    }
   ],
   "source": [
    "## import threading\n",
    "import time\n",
    "#def detect_hand():\n",
    "    \n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import load_model\n",
    "print(\"[INFO] sampling THREADED frames from webcam...\")\n",
    "vs = cv2.VideoCapture(0)\n",
    "detect_hand = True\n",
    "\n",
    "frames = np.zeros(shape=(36,75,100,3))\n",
    "i = 0\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    \n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "            \n",
    "            model = load_model('../../simple_model/best_model_6_supersimple.h5')\n",
    "            model._make_predict_function() \n",
    "            print(\"Ready to start\")\n",
    "            while True:\n",
    "                \n",
    "                ret,frame = vs.read()\n",
    "                frame = cv2.flip(frame,1)\n",
    "                frame = imutils.resize(frame, width=600)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # check to see if the frame should be displayed to our screen\n",
    "                if detect_hand == True:\n",
    "                    \n",
    "                    image_np = frame\n",
    "                    image_np_expanded = np.expand_dims(image_np,axis=0)\n",
    "                    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "                    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "                    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "                    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "                    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "                      \n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                    (boxes, scores, classes, num_detections) = sess.run(\n",
    "                    [boxes, scores, classes, num_detections],\n",
    "                    feed_dict={image_tensor: image_np_expanded})\n",
    "                    \n",
    "                    # Visualization of the results of a detection.\n",
    "                    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                        image_np,\n",
    "                        np.squeeze(boxes),\n",
    "                        np.squeeze(classes).astype(np.int32),\n",
    "                        np.squeeze(scores),\n",
    "                        category_index,\n",
    "                        use_normalized_coordinates=True,\n",
    "                        max_boxes_to_draw = 2,\n",
    "                        min_score_thresh=0.90,\n",
    "                        line_thickness=8)\n",
    "                    \n",
    "                    cv2.imshow(\"Frame\", frame)\n",
    "                    \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    \n",
    "                    condition = scores[0][0]*100                    \n",
    "                        \n",
    "                    if(condition >= 90):\n",
    "                        print(condition)\n",
    "                        detect_hand = False\n",
    "                    \n",
    "                    #frames_to_see.append(frame)\n",
    "                            \n",
    "                            \n",
    "                #começa a processar o gesto\n",
    "                if(detect_hand == False):\n",
    "                    \n",
    "                    cv2.imshow(\"Frame\", frame)\n",
    "                    \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "                    frame = cv2.resize(frame, (100,75))\n",
    "                    frames[i] =  frame\n",
    "                    i +=1;\n",
    "                    \n",
    "                    if(i == 36):\n",
    "                        l = model.predict([[frames]])\n",
    "                        l = np.argmax(l)\n",
    "                        if(l == 0): print(\"Doing other things\")\n",
    "                        elif(l==1): print(\"Swipping down\")\n",
    "                        elif(l==2): print(\"Swipping left\")\n",
    "                        elif(l==3): print(\"Zooming\")   \n",
    "                        detect_hand = True\n",
    "                        i=0\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                        print(\"Ready to capute the hand\")\n",
    "                        \n",
    "            # do a bit of cleanup\n",
    "            #vs.stream.release()\n",
    "            vs.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-13-33be04a246d9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-33be04a246d9>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    l = np.argmax(model.predict([[frames]]))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "## def predict(frames): \n",
    "    l = np.argmax(model.predict([[frames]]))  \n",
    "    print(model.predict([[frames]]))\n",
    "    if(l == 0): print(\"Doing other things\")\n",
    "    elif(l==1): print(\"Swipping down\")\n",
    "    elif(l==2): print(\"Swipping left\")\n",
    "    elif(l==3): print(\"Zooming\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(frames)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
