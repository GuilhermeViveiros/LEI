{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget to change the paths of your object detection file, it's too big to put on git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#840\n",
    "\n",
    "from imutils.video import WebcamVideoStream\n",
    "from imutils.video import FPS\n",
    "import os\n",
    "import imutils\n",
    "from IPython.display import display\n",
    "from cv2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15\n",
    "#!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow                             2.0.0      \n",
      "tensorflow-estimator                   2.0.1      \n",
      "tensorflow-object-detection-api        0.1.1      \n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow==2.00\n",
    "!pip list | grep tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/guilhermeviveiros/Desktop/models/research/object_detection\"\n",
    "MODEL_NAME = 'hand_graph'\n",
    "PATH_TO_CKPT = path + '/' + MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join(path+'/training', 'labelmap.pbtxt')\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.compat.v1.get_default_graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.compat.v1.GraphDef()\n",
    "    with tf.compat.v2.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "label_map_util.tf = tf.compat.v1\n",
    "tf.gfile = tf.io.gfile\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_several_images(PATH_TO_TEST_IMAGES_DIR = \"/Users/guilhermeviveiros/Desktop/Part_01/469\"):\n",
    "    \n",
    "    TEST_IMAGE_PATHS = []\n",
    "\n",
    "    for i in range(1,33):\n",
    "        if(i < 10):\n",
    "            path = os.path.join(PATH_TO_TEST_IMAGES_DIR, '0000{}.jpg'.format(i))\n",
    "            TEST_IMAGE_PATHS.append(path)\n",
    "        else:\n",
    "            path = os.path.join(PATH_TO_TEST_IMAGES_DIR, '000{}.jpg'.format(i))\n",
    "            TEST_IMAGE_PATHS.append(path)      \n",
    "                            \n",
    "    frames = []\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            for image_path in TEST_IMAGE_PATHS:\n",
    "               \n",
    "                image = cv2.imread(image_path)\n",
    "            \n",
    "                image = imutils.resize(image, width=600)\n",
    "                #image_np = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                image = cv2.Canny(image,100,200) \n",
    "                \n",
    "               \n",
    "                image_np_expanded = np.expand_dims(image_np,axis=0)\n",
    "                image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "                boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "                scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "                classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "                num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "                (boxes, scores, classes, num_detections) = sess.run(\n",
    "                    [boxes, scores, classes, num_detections],\n",
    "                    feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "                # Visualization of the results of a detection.\n",
    "                vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                          image_np,\n",
    "                          np.squeeze(boxes),\n",
    "                          np.squeeze(classes).astype(np.int32),\n",
    "                          np.squeeze(scores),\n",
    "                          category_index,\n",
    "                          use_normalized_coordinates=True,\n",
    "                          max_boxes_to_draw = 1,\n",
    "                          min_score_thresh=0.7,\n",
    "                          line_thickness=8)\n",
    "            \n",
    "                \n",
    "\n",
    "                frames.append(image)\n",
    "        \n",
    "            save_frames(frames)\n",
    "\n",
    "##if you wanna test the model in several images run the function bellow but change the path            \n",
    "##test_several_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames):\n",
    "    \n",
    "    for (step,frame) in enumerate(frames):\n",
    "        \n",
    "        if step < 10:\n",
    "            imwrite(\"/Users/guilhermeviveiros/Desktop/LEI/Task1/Fotos/0\" + str(step) + \".jpg\", frame)\n",
    "        else:\n",
    "            imwrite(\"/Users/guilhermeviveiros/Desktop/LEI/Task1/Fotos/\" + str(step) + \".jpg\", frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENCV WITHOUT THREADING FOR CAPTURING \n",
    "## grab a pointer to the video stream and initialize the FPS counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# created a *threaded* video stream, allow the camera sensor to warmup,\n",
    "# and start the FPS counter\n",
    "\n",
    "\n",
    "print(\"[INFO] sampling THREADED frames from webcam...\")\n",
    "vs = cv2.VideoCapture(0)\n",
    "i = 0\n",
    "\n",
    "\n",
    "#with detection_graph.as_default():\n",
    "#        with tf.Session(graph=detection_graph) as sess:\n",
    "#g = tf.Graph()\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    \n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        while True:\n",
    "            \n",
    "            ret,frame = vs.read()\n",
    "            frame = cv2.flip(frame,1)\n",
    "            frame = imutils.resize(frame, width=600)\n",
    "            #frame = cv2.resize(frame, (400,400))\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "            image_np = frame\n",
    "            #edges = cv2.Canny(frame,100,200) \n",
    "            image_np_expanded = np.expand_dims(image_np,axis=0)\n",
    "    \n",
    "            \n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            \n",
    "            (boxes, scores, classes, num_detections) = sess.run(\n",
    "            [boxes, scores, classes, num_detections],\n",
    "                feed_dict={image_tensor: image_np_expanded})\n",
    "                    \n",
    "                                    \n",
    "            # Visualization of the results of a detection.\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw = 2,\n",
    "                min_score_thresh=0.9,\n",
    "                line_thickness=8)\n",
    "                    \n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "                    \n",
    "                \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "        # do a bit of cleanup\n",
    "        #vs.stream.release()\n",
    "        vs.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cunny Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV program to perform Edge detection in real time \n",
    "# import libraries of python OpenCV  \n",
    "# where its functionality resides \n",
    "import cv2  \n",
    "  \n",
    "# np is an alias pointing to numpy library \n",
    "import numpy as np \n",
    "  \n",
    "  \n",
    "# capture frames from a camera \n",
    "cap = cv2.VideoCapture(0) \n",
    "  \n",
    "  \n",
    "# loop runs if capturing has been initialized \n",
    "while(1): \n",
    "  \n",
    "    # reads frames from a camera \n",
    "    ret, frame = cap.read() \n",
    "  \n",
    "    # converting BGR to HSV \n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV) \n",
    "      \n",
    "    # define range of red color in HSV \n",
    "    lower_red = np.array([30,150,50]) \n",
    "    upper_red = np.array([255,255,180]) \n",
    "      \n",
    "    # create a red HSV colour boundary and  \n",
    "    # threshold HSV image \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red) \n",
    "  \n",
    "    # Bitwise-AND mask and original image \n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask) \n",
    "  \n",
    "    # Display an original image \n",
    "    cv2.imshow('Original',frame) \n",
    "    frame = cv2.flip(frame,1)\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "        \n",
    "    # finds edges in the input image image and \n",
    "    # marks them in the output map edges \n",
    "    edges = cv2.Canny(frame,100,200) \n",
    "  \n",
    "    # Display edges in a frame \n",
    "    cv2.imshow('Edges',edges) \n",
    "  \n",
    "    # Wait for Esc key to stop \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "                    \n",
    "  \n",
    "  \n",
    "# Close the window \n",
    "cap.release() \n",
    "  \n",
    "# De-allocate any associated memory usage \n",
    "cv2.destroyAllWindows()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just an example of our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sampling THREADED frames from webcam...\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Ready to start\n",
      "94.38413977622986\n",
      "Swipping down\n",
      "Ready to capute the hand\n",
      "93.51779222488403\n",
      "Swipping down\n",
      "Ready to capute the hand\n",
      "93.54673624038696\n",
      "Zooming\n",
      "Ready to capute the hand\n",
      "93.21122765541077\n",
      "Swipping down\n",
      "Ready to capute the hand\n",
      "98.78085851669312\n",
      "Zooming\n",
      "Ready to capute the hand\n",
      "95.32358646392822\n",
      "Zooming\n",
      "Ready to capute the hand\n",
      "92.37041473388672\n",
      "Zooming\n",
      "Ready to capute the hand\n",
      "94.8197603225708\n",
      "Zooming\n",
      "Ready to capute the hand\n",
      "99.80244040489197\n",
      "Doing other things\n",
      "Ready to capute the hand\n",
      "97.39797115325928\n",
      "Zooming\n",
      "Ready to capute the hand\n",
      "97.89173603057861\n",
      "Doing other things\n",
      "Ready to capute the hand\n",
      "97.18940854072571\n",
      "Zooming\n",
      "Ready to capute the hand\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1fac8316c7d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     (boxes, scores, classes, num_detections) = sess.run(\n\u001b[1;32m     46\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_detections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     feed_dict={image_tensor: image_np_expanded})\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;31m# Visualization of the results of a detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## import threading\n",
    "import time\n",
    "#def detect_hand():\n",
    "    \n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import load_model\n",
    "print(\"[INFO] sampling THREADED frames from webcam...\")\n",
    "vs = cv2.VideoCapture(0)\n",
    "detect_hand = True\n",
    "\n",
    "frames = np.zeros(shape=(36,75,100,3))\n",
    "i = 0\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    \n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "            \n",
    "            model = load_model('../../simple_model/best_model_6_supersimple.h5')\n",
    "            model._make_predict_function() \n",
    "            print(\"Ready to start\")\n",
    "            while True:\n",
    "                \n",
    "                ret,frame = vs.read()\n",
    "                frame = cv2.flip(frame,1)\n",
    "                frame = imutils.resize(frame, width=600)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # check to see if the frame should be displayed to our screen\n",
    "                if detect_hand == True:\n",
    "                    \n",
    "                    image_np = frame\n",
    "                    image_np_expanded = np.expand_dims(image_np,axis=0)\n",
    "                    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "                    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "                    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "                    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "                    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "                      \n",
    "                    \n",
    "                        \n",
    "                        \n",
    "                    (boxes, scores, classes, num_detections) = sess.run(\n",
    "                    [boxes, scores, classes, num_detections],\n",
    "                    feed_dict={image_tensor: image_np_expanded})\n",
    "                    \n",
    "                    # Visualization of the results of a detection.\n",
    "                    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                        image_np,\n",
    "                        np.squeeze(boxes),\n",
    "                        np.squeeze(classes).astype(np.int32),\n",
    "                        np.squeeze(scores),\n",
    "                        category_index,\n",
    "                        use_normalized_coordinates=True,\n",
    "                        max_boxes_to_draw = 2,\n",
    "                        min_score_thresh=0.90,\n",
    "                        line_thickness=8)\n",
    "                    \n",
    "                    cv2.imshow(\"Frame\", frame)\n",
    "                    \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    \n",
    "                    condition = scores[0][0]*100                    \n",
    "                        \n",
    "                    if(condition >= 90):\n",
    "                        print(condition)\n",
    "                        detect_hand = False\n",
    "                    \n",
    "                    #frames_to_see.append(frame)\n",
    "                            \n",
    "                            \n",
    "                #começa a processar o gesto\n",
    "                if(detect_hand == False):\n",
    "                    \n",
    "                    cv2.imshow(\"Frame\", frame)\n",
    "                    \n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "                    frame = cv2.resize(frame, (100,75))\n",
    "                    frames[i] =  frame\n",
    "                    i +=1;\n",
    "                    \n",
    "                    if(i == 36):\n",
    "                        l = model.predict([[frames]])\n",
    "                        l = np.argmax(l)\n",
    "                        if(l == 0): print(\"Doing other things\")\n",
    "                        elif(l==1): print(\"Swipping down\")\n",
    "                        elif(l==2): print(\"Swipping left\")\n",
    "                        elif(l==3): print(\"Zooming\")   \n",
    "                        detect_hand = True\n",
    "                        i=0\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                        print(\"Ready to capute the hand\")\n",
    "                        \n",
    "            # do a bit of cleanup\n",
    "            #vs.stream.release()\n",
    "            vs.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def predict(frames): \n",
    "    l = np.argmax(model.predict([[frames]]))  \n",
    "    print(model.predict([[frames]]))\n",
    "    if(l == 0): print(\"Doing other things\")\n",
    "    elif(l==1): print(\"Swipping down\")\n",
    "    elif(l==2): print(\"Swipping left\")\n",
    "    elif(l==3): print(\"Zooming\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]]\n",
      "Swipping down\n"
     ]
    }
   ],
   "source": [
    "predict(frames)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
